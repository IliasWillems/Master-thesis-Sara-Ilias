bsplines <- lm(prestige~ns(income, df = 9), data = Prestige)
lines(seqx, predict(bsplines, data.frame(income = seqx)))
#install.packages("copula")
library(copula)
?mvdc
myCop <- normalCopula(param=0.75, dim = 2)
myMvdX <- mvdc(copula=myCop, margins=c("exp", "exp"),
paramMargins=list(list(rate=4),
list(rate=2)))
X <- rMvdc(5000,myMvdX)
myMvdY <- mvdc(copula=myCop, margins=c("norm", "norm"),
paramMargins=list(list(mean=0, sd=2),
list(mean=0, sd=1)))
Y <- rMvdc(5000,myMvdY)
par(mfrow=c(1,2))
plot(X,xlab="X1",ylab="X2",main="observations")
plot(Y,xlab="Y1",ylab="Y2",main="observations")
#(U1=FX1(X1),FX2(X2)) (probability integral transform)
UX=cbind(pexp(X[,1],rate=4), pexp(X[,2],rate=2))
# Apply the marginal cumulative distribution functions to the two entries of
# the random variables to get the uniformly distributed pseudo-observations.
UX=cbind(pexp(X[,1],rate=4), pexp(X[,2],rate=2))
UY=cbind(pnorm(Y[,1],mean=0,sd=2), pnorm(Y[,2],mean=0,sd=1))
plot(UX,xlab="UX1",ylab="UX2",main='ranks')
plot(UY,xlab="UY1",ylab="UY2",main='ranks')
#Different way to obtain such a sample
u <- rCopula(5000,myCop)
# Different way to obtain such a sample.
# --> Sample from a copula directly
u <- rCopula(5000,myCop)
plot(u,main="sample from copula")
plot(UY,xlab="UY1",ylab="UY2",main='ranks')
#Different copula parameters
par(mfrow=c(2,3))
parametervector=c(-0.7,-0.5,0,0.5,0.7,0.9)
for (i in 1:length(parametervector) ){
myCop <- normalCopula(param=parametervector[i], dim = 2)
myMvdX <- mvdc(copula=myCop, margins=c("exp", "exp"),
paramMargins=list(list(rate=4),
list(rate=2)))
X <- rMvdc(5000,myMvdX)
plot(X,main=paste("rho = ", parametervector[i]),xlab="X1",ylab="X2")
}
#Different copula parameters
par(mfrow=c(2,3))
parametervector=c(-0.7,-0.5,0,0.5,0.7,0.9)
#Different copula parameters
par(mfrow=c(2,3))
parametervector=c(-0.7,-0.5,0,0.5,0.7,0.9)
for (i in 1:length(parametervector) ){
myCop <- normalCopula(param=parametervector[i], dim = 2)
myMvdX <- mvdc(copula=myCop, margins=c("norm", "norm"),
paramMargins=list(list(rate=4),
list(rate=2)))
X <- rMvdc(5000,myMvdX)
plot(X,main=paste("rho = ", parametervector[i]),xlab="X1",ylab="X2")
}
#Different copulas
fgm = fgmCopula(dim = 2, param = 0.8) #Farlie-Gumbel-Morgenstein copula
amh = amhCopula(dim = 2, param = 0.8) #Ali-Mikhail-Haq copula
ind = indepCopula(dim=2) #independence copula
studT=tCopula(param=0.8,dim=2,df=2) #student t copula
normal=normalCopula(param=0.8,dim=2) #Gaussian copula
counter=lowfhCopula(dim=2) #countermonotonicity copula (perfect negative dependence)
copulavector=list(fgm,amh,ind,studT,normal,counter)
names(copulavector)=list("fgm","ahm","Independence","student t", "Gaussian","countermonotonic")
par(mfrow=c(2,3))
for (i in 1:length(copulavector) ){
myMvdX <- mvdc(copula=copulavector[[i]], margins=c("exp", "exp"),
paramMargins=list(list(rate=4),
list(rate=2)))
X <- rMvdc(5000,myMvdX)
plot(X,main=paste("copula = ", names(copulavector)[i]),xlab="X1",ylab="X2")
}
mtext("Observations", side =3,line = -2, outer = TRUE,col='red')
#Plot copula
par(mfrow=c(1,3))
normal=normalCopula(0.8)
indep=indepCopula()
counter=lowfhCopula(dim=2)
copulavector=list(normal,indep,counter)
names(copulavector)=c("normal","independence","countermonotonic")
#samples from copulas
for (i in 1:length(copulavector) ){
u <- rCopula(5000,copulavector[[i]])
plot(u,main=paste("copula = ", names(copulavector)[i]))
}
mtext("Sample from copulas", side =3,line = -2, outer = TRUE,col='red')
#cdf of copula
par(mfrow=c(2,3))
#3d plot
for (i in 1:length(copulavector) ){
persp(copulavector[[i]],pCopula,col='lightgreen',main=paste("copula = ", names(copulavector)[i]))
}
mtext("cdf of copulas", side =3,line = -2, outer = TRUE,col='red')
#contour plot
for (i in 1:length(copulavector) ){
contour(copulavector[[i]],pCopula,col='lightgreen',main=paste("copula = ", names(copulavector)[i]))
}
mtext("cdf of copulas", side =3,line = -2, outer = TRUE,col='red')
#density
par(mfrow=c(1,2))
persp(normal,dCopula,col='lightgreen',main="Gaussian")
persp(indep,dCopula,col='lightgreen',main="Independence")
mtext("density of copulas", side =3,line = -2, outer = TRUE,col='red')
##Kendall's tau
fgm = fgmCopula(dim = 2, param = 0.8) #Farlie-Gumbel-Morgenstein copula
amh = amhCopula(dim = 2, param = 0.8) #Ali-Mikhail-Haq copula
ind = indepCopula(dim=2) #independence copula
studT=tCopula(param=0.8,dim=2,df=2) #student t copula
normal=normalCopula(param=0.8,dim=2) #Gaussian copula
counter=lowfhCopula(dim=2) #countermonotonicity copula (perfect negative dependence)
copulavector=list(fgm,amh,ind,studT,normal,counter)
for (i in 1:length(copulavector) ){
myMvdX <- mvdc(copula=copulavector[[i]], margins=c("exp", "exp"),
paramMargins=list(list(rate=4),
list(rate=2)))
X <- rMvdc(5000,myMvdX)
print(cor(X[,1],X[,2],method="kendall"))
}
#Multivariate case (k>2)
#install.packages("scatterplot3d")
library("scatterplot3d")
par(mfrow=c(2,2))
myCop <- normalCopula(param=c(0.9,-0.5,-0.2), dim = 3,dispstr="un")
myCop <- normalCopula(param=c(0.9,-0.5,-0.2), dim = 3, dispstr="un")
myMvd <- mvdc(copula=myCop, margins=c("exp", "norm", "norm"),
paramMargins=list(list(rate=1),
list(mean=5,sd=4),
list(mean=0,sd=2)) )
X<- rMvdc(2000,myMvd)
colnames(X) <- c("x1", "x2", "x3")
plot(X[,1],X[,2],pch="*")
plot(X[,1],X[,3],pch="*")
plot(X[,2],X[,3],pch="*")
scatterplot3d(X,box=F,pch="*")
#sample from Copula
U <- rCopula(2000,myCop)
plot(U[,1],U[,2],pch="*")
plot(U[,1],U[,3],pch="*")
plot(U[,2],U[,3],pch="*")
scatterplot3d(U,box=F,pch="*")
?mvdc
simulateSample <- function(x, theta)
{
n = length(x)
Y=c()
for (i in 1:n){
myCop=claytonCopula(dim=2,param = theta(x[i]))
myMvdX <- mvdc(copula=myCop, margins=c("norm", "norm"),
paramMargins=list(list(mean=x[i],sd=1),
list(mean=0,sd=x[i])))
newrows <- cbind(x[i],rMvdc(1,myMvdX))
Y=rbind(Y,newrows)
}
return (Y)
}
#Let X be exponentially distributed with lambda=2
#Take sample from X, for each X=x in sample we can now obtain an observations for (Y1,Y2)
par(mfrow=c(1,1))
Xsample=rexp(500,2)
w=simulateSample(x=Xsample,theta=identity)
w
colnames(w)=c("X","Y1","Y2")
scatterplot3d(w)
#install.packages("kdecopula")
library("kdecopula")
?kdecop #see https://cran.r-project.org/web/packages/kdecopula/vignettes/kdecopula.pdf
par(mfrow=c(1,1))
#construct data
myCop <- normalCopula(param=c(0.75), dim = 2)
myMvdX <- mvdc(copula=myCop, margins=c("norm", "norm"),
paramMargins=list(list(mean=0,sd=1),
list(mean=2,sd=2)))
X <- rMvdc(500,myMvdX)
U <- apply(X, 2, rank) / (nrow(X) + 1) #NOTE: devide by n+1 instead of n
Xsample
#construct data
myCop <- normalCopula(param=c(0.75), dim = 2)
myMvdX <- mvdc(copula=myCop, margins=c("norm", "norm"),
paramMargins=list(list(mean=0,sd=1),
list(mean=2,sd=2)))
X <- rMvdc(500,myMvdX)
U <- apply(X, 2, rank) / (nrow(X) + 1) #NOTE: devide by n+1 instead of n
#Built in way to obtain same result:
U=pobs(X)
fitMR <- kdecop(U,method="MR") #mirror-reflection estimator of Gijbels and Mielniczuk (1990)
summary(fitMR)
dkdecop(c(0.1, 0.2), fitMR) #evaluate density
pkdecop(c(0.1, 0.2), fitMR) #evaluate cdf
plot(fitMR,xlim=c(0,1),ylim=c(0,1)) #density   (PLOT A)
contour(fitMR,margins = "unif") #contour plot of the copula density (PLOT B)
contour(fitMR,xlab="gaussian 1", ylab="gaussian 2") #a contour plot of the copula density when combined with standard normal margins. (PLOT C)
plot(fitMR,xlim=c(0,1),ylim=c(0,1)) #density   (PLOT A)
contour(fitMR,margins = "unif") #contour plot of the copula density (PLOT B)
contour(fitMR,xlab="gaussian 1", ylab="gaussian 2") #a contour plot of the copula density when combined with standard normal margins. (PLOT C)
#to replicate plot C:
x<- seq(-3,3,0.1)
#to replicate plot C:
x<- seq(-3,3,0.1)
y <- seq(-3,3,0.1)
DF <- expand.grid(x,y)
Z=rep(0,nrow(DF))
for (i in 1:nrow(DF)){
Z[i]=dkdecop(c(pnorm(DF$Var1[i]),pnorm(DF$Var2[i])), fitMR)*dnorm(DF$Var1[i])*dnorm(DF$Var2[i])
}
Mat <- matrix(Z,nrow = 61)
contour(x,y,z=Mat,levels=c(0.2,0.15,0.1,0.05,0.025,0.01))
#Other methods to estimate copula
fitbeta <- kdecop(U,method="beta")#The beta kernel estimator of Charpentier et al. (2006)
fitT <- kdecop(U,method="T")#The transformation estimator Geenenens et al., 2014
fitBern<-kdecop(U,method="bern")#Bernstein copula estimator by Sanchetta and Satchell, 2004
#Plotting all methods when using uniform margins
par(mfrow=c(2,3))
contour(fitMR,margins = "unif",main="MR")
contour(fitbeta,margins = "unif",main="beta")
contour(fitT,margins = "unif",main="T")
contour(fitBern,margins = "unif",main="Bern")
contour(normalCopula(0.75),dCopula,main="True copula")
#Using normal margins
par(mfrow=c(2,3))
contour(fitMR,main="MR",xlab="gaussian 1", ylab="gaussian 2")
contour(fitbeta,main="beta",xlab="gaussian 1", ylab="gaussian 2")
contour(fitT,main="T",xlab="gaussian 1", ylab="gaussian 2")
contour(fitBern,main="Bern",xlab="gaussian 1", ylab="gaussian 2")
myCop <- normalCopula(param=c(0.75), dim = 2)
myMvdX <- mvdc(copula=myCop, margins=c("norm", "norm"),
paramMargins=list(list(mean=0,sd=1),
list(mean=0,sd=1)))
contour(myMvdX,dMvdc,xlim=c(-3,3),ylim=c(-3,3),levels=c(0.2,0.15,0.1,0.05,0.025,0.01),main="True copula",xlab="gaussian 1", ylab="gaussian 2")
#Parametric estimation (inverting Kendall's tau)
gaussian.cop=normalCopula(0.5,dim=2) #0.5 is just to fill something in, doesn't matter
fit.tau <- fitCopula(gaussian.cop, U, method="itau")
fit.tau #0.7453 is close to true parameter which is 0.75
U
?apply
?rank
#////////// conditional copulas /////////////#
par(mfrow=c(1,1))
library("locpol")
Xsample=rexp(100,2)
#Let us use again the previously defined function simulateSample
TotalSample=simulateSample(x=Xsample,theta=identity)
X=TotalSample[,1]
Y1=TotalSample[,2]
Y2=TotalSample[,3]
library("scatterplot3d")
scatterplot3d(TotalSample)
#Define function that calculates this (using locpol (local linear if deg=1))
#In particular we want to estimate F1x(Yj).
#Y=unconditionaldata
#X=conditionaldata
#Yj=Ypoint
#x=Xpoint
conditionalCdfEstimator=function(unConditionaldata,conditionaldata,Ypoint,Xpoint){
d=data.frame(x=conditionaldata)
d$y=as.numeric(unConditionaldata<Ypoint)
return(as.numeric(locpol(y~x,data =d ,kernel=gaussK,xeval=Xpoint,deg=1)$lpFit[2]))
}
#Use this function: loop over Y1 (similar for Y2) using i as index and
#set Ypoint=Y1[i], xPoint=X[i]
U1=c()
U2=c()
for (i in 1:length(X)){
U1[i]=conditionalCdfEstimator(Y1,X,Y1[i],X[i]) #U1i=F1Xi(Y1i)
U2[i]=conditionalCdfEstimator(Y2,X,Y2[i],X[i]) #U2i=F2Xi(Y2i)
}
#Estimate Cx(u1,u2) again using Local linear estimator.
#Indeed: we now want to estimate Cx(u1,u2)=P(U1<=u1,U2<=u2 |X=x)=E[I{U1<=u1,U2<=u2}| X=x]
#Using the notation from above we see that this can be done using local linear estimator with
#~~>Y*=I{U1<=u1,U2<=u2}=I{U1<=u1}*I{U2<=u2}
#~~~>X*=X
#~~~>x*=x
#To make this more elegant lets again define a function
conditionalCopulaEstimator=function(U1,U2,u1,u2,X,x){
d=data.frame(x=X)
d$y=as.numeric((U1<u1)*(U2<u2))
return(as.numeric(locpol(y~x,data =d ,kernel=gaussK,xeval=x,deg=1)$lpFit[2]))
}
#Estimate Cx(u1,u2) again using Local linear estimator.
#Indeed: we now want to estimate Cx(u1,u2)=P(U1<=u1,U2<=u2 |X=x)=E[I{U1<=u1,U2<=u2}| X=x]
#Using the notation from above we see that this can be done using local linear estimator with
#~~>Y*=I{U1<=u1,U2<=u2}=I{U1<=u1}*I{U2<=u2}
#~~~>X*=X
#~~~>x*=x
#To make this more elegant lets again define a function
conditionalCopulaEstimator=function(U1,U2,u1,u2,X,x){
d=data.frame(x=X)
d$y=as.numeric((U1<u1)*(U2<u2))
return(as.numeric(locpol(y~x,data =d ,kernel=gaussK,xeval=x,deg=1)$lpFit[2]))
}
#Plot a contour plot of our estimator Cx(u,v) for x=0.5
par(mfrow=c(1,1))
scatterplot3d(TotalSample)
#make a grid
par(mfrow=c(1,2))
x<- seq(0,1,0.1)
y <- seq(0,1,0.1)
DF <- expand.grid(x,y) #makes a spiral/snake looping through your grid points: plot(DF[,1],DF[,2],type="l")
Z=rep(0,nrow(DF)) #For each point in snake you want Cx(u,v). So here initialize Z
for (i in 1:nrow(DF)){
Z[i]=conditionalCopulaEstimator(U1,U2,DF$Var1[i],DF$Var2[i],X,0.5)
}
Mat <- matrix(Z,nrow = length(x)) #"wrap snake up again"
contour(x,y,z=Mat)
contour(claytonCopula(dim=2,param = 0.5),pCopula) #True copula and parameter
#-------------------------------------------------------#
# --------- Estimating conditional kendall tau ---------#
#-------------------------------------------------------#
library("evmix")
#Make a sample (again using simulateSample)
Xsample=rnorm(mean=5,sd=1,500)
TotalSample=simulateSample(x=Xsample,theta=identity)
X=TotalSample[,1]
Y1=TotalSample[,2]
Y2=TotalSample[,3]
library("scatterplot3d")
scatterplot3d(TotalSample)
#calculate conditional kendall tau in point x
#tau(x)=4/(1-sum_{i=1}^n w_{ni}(x,h)^2))*sum_{i=1}^n sum_{j=1}^n w_{ni}(x,h)*w_{nj}(x,h)I{Y1i<Y1j,Y2i<Y2j}
cond.tau.estim <- function(Y1, Y2, X, x0, hn){
#   Y1 ... values of the (possible adjusted) first coordination of the response
#   Y2 ... values of the (possible adjusted) second coordination of the response
#   X ... values of the univariate covariate
#   x0 ... values of the covariate where cond. tau should be estimated
#   hn ... bandwidth to be used in the calculation of the cond. tau
# #   Sorting of the values of x0
x0 <- sort(x0);
n <- length(X);
nx <- length(x0);
YY1 <- outer(Y1, Y1, "<");
YY2 <- outer(Y2, Y2, "<");
YY <- YY1 * YY2; #YY(i,j)=I{Y1i<Y1j,Y2i<Y2j}
# #   Weights with the help of locpol package
Wx0<- locLinWeightsC(X, xeval=x0, bw=hn, kernel=TriweigK)$locWeig;
Wx0
tau.estim <- numeric(nx);
for(i in 1:nx){
wn <- Wx0[i,];
W <- outer(wn, wn); #W(i,j)=wni*wnj
w2 <- sum(wn^2);
tau.estim[i] <-  4/(1-w2) * (sum(YY * W)) - 1;#(YY*W)(i,j)=YY(i,j)*W(i,j). Sum function sums everything in matrix
}
print("------------------------------------------------------------");
print(paste("Sample size :", n));
print("");
print("");
print("Estimation of Kendall\'s tau");
print("");
print(tau.estim, digits=2);
return(cbind(x0, tau.estim));
}
# Since it does not make sense to compute the Kendall's tau for x-values of
# which you do not have any data points, we only look at x-values between 3.5
# and 7.
xx = seq(3.5,7, 0.1)
estimatedConditionalKendall=xx
trueConditionalKendall=xx
estimatedUnconditionalKendall=rep(cor(Y1,Y2,method="kendall"),length(xx))
for (j in 1:length(xx)){
estimatedConditionalKendall[j]=cond.tau.estim(Y1,Y2,X,xx[j],0.6)[2]
trueConditionalKendall[j]=tau(claytonCopula(xx[j]))
}
plot(xx,estimatedConditionalKendall,xlab="x",type="l",main="(un)conditional Kendall tau",xlim=c(3.5,7),ylim=c(0.4,1))
lines(xx,trueConditionalKendall,col="red")
lines(xx,estimatedUnconditionalKendall,col="blue")
legend("topleft",legend=c("estimated conditional","true conditional","unconditional"),col=c("black","red","blue"),lty=1)
library(copula)
?claytonCopula
myCopula <- claytonCopula(param = 2, dim = 2)
?mvdc
myMvdc <- mvdc(copula = myCopula, margins = c("norm", "exp"),
paramMargins = list(list(0, 1), list(1)))
random.sample <- rmvdc(1000, myMvdc)
random.sample <- rMvdc(1000, myMvdc)
random.sample1 <- rMvdc(1000, myMvdc)
myMvdc2 <- mvdc(copula = myCopula, margins = c("exp", "exp"),
paramMargins = list(list(2), list(3)))
random.sample2 <- rMvdc(1000, myMvdc2)
plot(random.sample1[,1], random.sample1[,2])
par(mfrow = c(1,2))
plot(random.sample2[,1], random.sample2[,2])
library(copula)
par(mfrow = c(1,2))
myCopula <- claytonCopula(param = 2, dim = 2)
myMvdc <- mvdc(copula = myCopula, margins = c("norm", "exp"),
paramMargins = list(list(0, 1), list(1)))
random.sample1 <- rMvdc(1000, myMvdc)
plot(random.sample1[,1], random.sample1[,2])
myMvdc2 <- mvdc(copula = myCopula, margins = c("exp", "exp"),
paramMargins = list(list(2), list(3)))
random.sample2 <- rMvdc(1000, myMvdc2)
plot(random.sample2[,1], random.sample2[,2])
#For the next questions we work with the rdj dataset from the copula package
#Five years of daily log-returns (from 1996 to 2000) of Intel (INTC), Microsoft (MSFT) and General Electric (GE) stocks
#For simplicity we assume that these log-returns are iid
library("copula")
plot(pnorm(random.sample1[,1]), pexp(random.sample[,2]))
plot(pexp(random.sample[1,]), pexp(random.sample2[,2]))
plot(pexp(random.sample2[1,]), pexp(random.sample2[,2]))
plot(pexp(random.sample2[,1]), pexp(random.sample2[,2]))
plot(pnorm(random.sample1[,1]), pexp(random.sample1[,2]))
plot(pexp(random.sample2[,1]), pexp(random.sample2[,2]))
#For the next questions we work with the rdj dataset from the copula package
#Five years of daily log-returns (from 1996 to 2000) of Intel (INTC), Microsoft (MSFT) and General Electric (GE) stocks
#For simplicity we assume that these log-returns are iid
library("copula")
data(rdj, package="copula")
X=rdj[,2:4]
?kdecop
fitMR.returns <- kdecop(X, method = "MR")
X
fitMR.returns <- kdecop(X[,1:2], method = "MR")
apply(X[,1:2], 2, rank)
X
apply(X[,1:2], 2, rank) / (nrow(X[,1:2]) + 1)
fitMR.returns <- kdecop(U.returns, method = "MR")
U.returns <- apply(X[,1:2], 2, rank) / (nrow(X[,1:2]) + 1)
fitMR.returns <- kdecop(U.returns, method = "MR")
contour(fitMR.returns)
contour(fitMR.returns, margins = "unif")
par(mfrow = c(1, 1))
U.returns <- apply(X[,1:2], 2, rank) / (nrow(X[,1:2]) + 1)
fitMR.returns <- kdecop(U.returns, method = "MR")
contour(fitMR.returns, margins = "unif")
cor(X[,1], X[,2], method = "kendall")
?frankCopula
myFrank <- frankCopula(param = 5, dim = 2)
fit.frank <- fitCopula(myFrank, U, method = "itau")
myGauss <- gaussianCopula(param = 5, dim = 2)
myGauss <- normalCopula(param = 5, dim = 2)
myGauss <- normalCopula(param = 5, dim = 2)
myGauss <- normalCopula(5, dim = 2)
myGauss <- normalCopula(0.5, dim = 2)
fit.gauss <- fitCopula(myGauss, U, method = "itau")
fit.frank
fit.gauss
cor(X[,1], X[,2], method = "kendall")
plot(fit.frank)
View(fit.frank)
?mvdc
View(fit.frank)
tm(list = sl())
tm(list = ls())
rm(list = ls())
library(MASS)
library(nloptr)
library(numDeriv)
library(xtable)
library(VGAM)
source("Functions_ad.R")
init.value.theta_1=1
init.value.theta_2=1
parN = list(beta=c(2.5,2.6,1.8,2),eta=c(1.8,0.9,0.5,-2.2),sd=c(1.1,1.4,0.75,1, 0.5),gamma=c(-1,0.6,2.3))    #45-50% censoring
parl = length(parN[[1]])
totparl = 2*parl
parlgamma = (parl-1)
namescoef =  c("beta_{T,0}","beta_{T,1}","alpha_T","lambda_T","beta_{C,0}","beta_{C,1}","alpha_C","lambda_C","sigma_T","sigma_C","rho","theta_1","theta_2")
samsize= c(250) #c(250, 500, 1000)
for(l in samsize)
k
n <- 10000
nsim <- 300
iseed <- 747852
# To split up simulation in different parts
number.of.parts <- 30
#16-30
parts.to.evaluate <- 1:15
for (part.to.evaluate in parts.to.evaluate) {
message("Evaluating part ", which(part.to.evaluate == parts.to.evaluate),
" out of ", length(parts.to.evaluate))
start.time <- Sys.time()
SimulationCI11_SaraIlias_Simplified(n, nsim, iseed, init.value.theta_1,
init.value.theta_2, part.to.evaluate,
number.of.parts)
diff <- start.time - Sys.time()
message("This iteration ran for approximately ")
message(round(diff))
message("")
}
Summarize_results()
# Some input validation
if (nsim %% number.of.parts != 0) {
stop("nsim needs to be a multiple of number.of.parts.")
}
if ((part.to.evaluate > number.of.parts) || (part.to.evaluate <= 0)) {
stop("part.to.evaluate is not valid.")
}
number.of.parts
part.to.evaluate
nsim
nsim <- 2500
number.of.parts <- 100
number.of.parts <- 250
# Some input validation
if (nsim %% number.of.parts != 0) {
stop("nsim needs to be a multiple of number.of.parts.")
}
if ((part.to.evaluate > number.of.parts) || (part.to.evaluate <= 0)) {
stop("part.to.evaluate is not valid.")
}
# Create the appropriate set of i's to check
part.size <- nsim / number.of.parts
i.to.check <- 1:nsim
i.to.check <- i.to.check[(part.size*(part.to.evaluate-1) + 1):(part.size*part.to.evaluate)]
i.to.checl
i.to.check
part.to.evaluate
"EC2 naive"
"EC1 estimated V"
"EC3 real V"
"EC4 independence"
